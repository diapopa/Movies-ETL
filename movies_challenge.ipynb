{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file path of the directory to reference later\n",
    "file_dir = 'C:/Users/Dianysus/Desktop/UofT/8-ETL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the wiki_movies file \n",
    "with open(f'{file_dir}/Movies-ETL/wikipedia.movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the kaggle csv files\n",
    "kaggle_metadata = pd.read_csv(f'{file_dir}/movies_metadata.csv', low_memory=False)\n",
    "ratings = pd.read_csv(f'{file_dir}/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the move file\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    # Make an empty dict to hold all the alternative titles\n",
    "    alt_titles = {}\n",
    "    # Loop through a list of all alternative title keys\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        # Check if the current key exists in the movie object\n",
    "        if key in movie:\n",
    "            # If so, remove the key-value pair and add to the alternative titles dictionary\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    # After looping through every key, add the alternative titles dict to the movie object\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "    \n",
    "     # merge column names\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "        \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    if type(s) != str:\n",
    "        return np.nan\n",
    "    # if input is of the form $###.# million\n",
    "    if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "        # remove dollar sign and \" million\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "        # convert to float and multiply by a million\n",
    "        value = float(s) * 10**6\n",
    "        # return value\n",
    "        return value\n",
    "    # if input is of the form $###.# billion\n",
    "    elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "        # remove dollar sign and \" billion\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "        # convert to float and multiply by a billion\n",
    "        value = float(s) * 10**9\n",
    "        # return value\n",
    "        return value\n",
    "    # if input is of the form $###,###,###\n",
    "    elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "        # remove dollar sign and commas\n",
    "        s = re.sub('\\$|,','', s)\n",
    "        # convert to float\n",
    "        value = float(s)\n",
    "        # return value\n",
    "        return value\n",
    "    # otherwise, return NaN\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that fills in missing data for a column pair and drops the redundant column\n",
    "def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "    df[kaggle_column] = df.apply(\n",
    "        lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "        , axis=1)\n",
    "    df.drop(columns=wiki_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL function\n",
    "def ETL(wiki_movies_raw, kaggle_metadata, ratings):\n",
    "    \n",
    "    # Create dataframes from the kaggle lists\n",
    "    kaggle_metadata_df = pd.DataFrame(kaggle_metadata) \n",
    "    ratings_df = pd.DataFrame(ratings)\n",
    "    \n",
    "    # Save the filtered data to a list\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "    \n",
    "    # Create a dataframe\n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies)\n",
    "    \n",
    "    # Make a list of cleaned movies with list comprehension\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "    # Set dataframe from clean_movies and print out a list of the columns\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "    sorted(wiki_movies_df.columns.tolist())\n",
    "\n",
    "    wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    \n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    \n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    box_office[box_office.map(lambda x: type(x) != str)]\n",
    "    \n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    form_one = r'\\$\\d+\\.?\\d*\\s*[mb]illion'\n",
    "    box_office.str.contains(form_one, flags=re.IGNORECASE).sum()\n",
    "    \n",
    "    form_two = r'\\$\\d{1,3}(?:,\\d{3})+'\n",
    "    box_office.str.contains(form_two, flags=re.IGNORECASE).sum()\n",
    "    \n",
    "    matches_form_one = box_office.str.contains(form_one, flags=re.IGNORECASE)\n",
    "    matches_form_two = box_office.str.contains(form_two, flags=re.IGNORECASE)\n",
    "    \n",
    "    box_office[~matches_form_one & ~matches_form_two]\n",
    "    \n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    \n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    box_office.str.extract(f'({form_one}|{form_two})')\n",
    "    \n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    \n",
    "    # Convert any lists to strings\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Remove any values between a dollar sign and a hyphen\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "\n",
    "    matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE)\n",
    "    matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE)\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    \n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    \n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # Make a variable that holds the non-null values of Release date in the DataFrame, converting lists to strings\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "    \n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    \n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE).sum()\n",
    "    \n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE) != True]\n",
    "    \n",
    "    running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE).sum()\n",
    "    \n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE) != True]\n",
    "    \n",
    "    running_time[running_time.str.contains(r'\\d*\\s*m', flags=re.IGNORECASE) != True] \n",
    "    \n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    \n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    \n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    \n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    kaggle_metadata[~kaggle_metadata['adult'].isin(['True','False'])]\n",
    "    \n",
    "    # Keep rows where the adult column is False and drop the adult column\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    \n",
    "    # Convert data types\n",
    "    try:\n",
    "        kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # Competing data:\n",
    "    # Wiki                     Movielens                Resolution\n",
    "    #--------------------------------------------------------------------------\n",
    "    # title_wiki               title_kaggle             Drop Wikipedia.\n",
    "    # running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "    # budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "    # box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "    # release_date_wiki        release_date_kaggle      Drop Wikipedia.\n",
    "    # Language                 original_language        Drop Wikipedia.\n",
    "    # Production company(s)    production_companies     Drop Wikipedia.\n",
    "    \n",
    "    # Check for missing titles in the kaggle data and fill them in with 0\n",
    "    movies_df[(movies_df['title_kaggle'] == '') | (movies_df['title_kaggle'].isnull())]\n",
    "    movies_df.fillna(0)\n",
    "\n",
    "    # Look for any movie with a release date after 1996 according to Wiki and before 1965 according to Kaggle\n",
    "    movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')]\n",
    "\n",
    "    # Get the index of that row\n",
    "    movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index\n",
    "\n",
    "    # Drop the row\n",
    "    movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "\n",
    "    # Drop the title_wiki, release_date_wiki, language, and Prof=duction company(s) columns\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    # Run the function for the three column pairs\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "    # Check for any columns with only one value\n",
    "    for col in movies_df.columns:\n",
    "        lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "        value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "        num_values = len(value_counts)\n",
    "        if num_values == 1:\n",
    "            print(f\"Only one value in column: {col}\")\n",
    "            \n",
    "    # Reorder columns\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "    \n",
    "    # Rename columns\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "    \n",
    "    # Use groupby on movieId and rating columns and take the count for each group.\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count()\n",
    "    \n",
    "    # Rename the userId column to count\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \n",
    "    \n",
    "    # Make movieId the index, the rating values the columns, and the counts for each rating the rows.\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "    \n",
    "    # Rename the columns\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    # Left merge to join the ratings counts onto movies_df\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    # Fill in missing values with zero\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    try:\n",
    "        db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "        engine = create_engine(db_string)\n",
    "    except e:\n",
    "        raise f\"Connection string failure - Error: {e}\"\n",
    "    \n",
    "    try:\n",
    "        movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    except e:\n",
    "        raise f\"Writing to databse failed - Error: {e}\"\n",
    "    # create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        try:\n",
    "            data.to_sql(name='ratings', con=engine, if_exists='replace')\n",
    "        except e:\n",
    "            raise f\"Writing to databse failed - Error: {e}\"\n",
    "        # increment the number of rows imported by the size of 'data'\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed') \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 282.75258469581604 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 565.7703001499176 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 831.4838442802429 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 1091.2484290599823 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 1346.2694156169891 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 1605.514743566513 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 1870.391126871109 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 2130.649971485138 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 2412.211116552353 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 2695.6988389492035 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 2968.6925599575043 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 3232.4265434741974 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 3503.307806968689 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 3776.711965560913 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 4036.7740335464478 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 4299.6948165893555 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 4589.520491600037 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 4909.326166391373 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 5234.5046265125275 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 5549.8731808662415 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 5851.789571523666 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 6105.928827762604 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 6371.489820957184 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 6638.691504001617 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 6909.4175090789795 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 7178.965095043182 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 7185.893089771271 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "ETL(wiki_movies_raw, kaggle_metadata, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
